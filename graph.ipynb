{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "091fa681-5248-4c2a-8770-98fd066cbc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6f8ff9d5-814e-43e0-9f0e-cca862012828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cpu'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b8c15e21-4b12-4c4f-90d4-094ac7ce4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bef5d87c-1b03-43a2-9e38-87a1b520b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "fen=\"5r1k/4b1p1/2p3qp/3n3r/2B1p3/4P1P1/Q2B1P1P/2R1R1K1 b - - 2 45\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "95e8aca9-70a3-4b0c-beac-be2e40c78a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "class FenParser():\n",
    "    def __init__(self, fen_str):\n",
    "        self.fen_str = fen_str\n",
    "\n",
    "    def parse(self):\n",
    "        ranks = self.fen_str.split(\" \")[0].split(\"/\")\n",
    "        pieces_on_all_ranks = [self.parse_rank(rank) for rank in ranks]\n",
    "        return pieces_on_all_ranks\n",
    "\n",
    "    def parse_rank(self, rank):\n",
    "        rank_re = re.compile(\"(\\\\d|[kqbnrpKQBNRP])\")\n",
    "        piece_tokens = rank_re.findall(rank)\n",
    "        pieces = self.flatten(map(self.expand_or_noop, piece_tokens))\n",
    "        return pieces\n",
    "\n",
    "    def flatten(self, lst):\n",
    "        return list(chain(*lst))\n",
    "\n",
    "    def expand_or_noop(self, piece_str):\n",
    "        piece_re = re.compile(\"([kqbnrpKQBNRP])\")\n",
    "        retval = \"\"\n",
    "        if piece_re.match(piece_str):\n",
    "          retval = piece_str\n",
    "        else:\n",
    "          retval = self.expand(piece_str)\n",
    "        \n",
    "        return retval\n",
    "\n",
    "    def expand(self, num_str):\n",
    "        return int(num_str)*\" \"\n",
    "\n",
    "    def white_to_move(self):\n",
    "        return True if self.fen_str.split()[1] == 'w' else False\n",
    "\n",
    "    def can_castle(self, color, side):\n",
    "        castling_rights = self.fen_str.split()[2]\n",
    "        \n",
    "        if castling_rights == '-':\n",
    "            return False\n",
    "\n",
    "        if color == 'w':\n",
    "            if side == 'k':\n",
    "                return True if 'K' in castling_rights else False\n",
    "            elif side == 'q':\n",
    "                return True if 'Q' in castling_rights else False\n",
    "\n",
    "        if color == 'b':\n",
    "            if side == 'k':\n",
    "                return True if 'k' in castling_rights else False\n",
    "            elif side == 'q':\n",
    "                return True if 'q' in castling_rights else False\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_board(self):\n",
    "        return chess.Board(self.fen_str)\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return self.get_board().legal_moves\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "83cf22b5-d34c-48e0-b1d8-700bfc44dce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>. . . . . r . k\n",
       ". . . . b . p .\n",
       ". . p . . . q p\n",
       ". . . n . . . r\n",
       ". . B . p . . .\n",
       ". . . . P . P .\n",
       "Q . . B . P . P\n",
       ". . R . R . K .</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\" /><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(105, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(195, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(285, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(15, 285)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(150, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 240)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 195)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(195, 195)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(150, 150)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(330, 150)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 105)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(285, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 105)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(195, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(240, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(330, 15)\" /></svg>"
      ],
      "text/plain": [
       "Board('5r1k/4b1p1/2p3qp/3n3r/2B1p3/4P1P1/Q2B1P1P/2R1R1K1 b - - 2 45')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = FenParser(fen)\n",
    "fp.get_board()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "77f5548c-2f0a-4396-aa8d-a86c4f094f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Index: tensor([[ 0,  0,  0,  ..., 63, 63, 63],\n",
      "        [10, 17,  0,  ..., 61, 62, 63]])\n",
      "Number of Edges: 1856\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Map square to (row, col) and vice versa\n",
    "def square_to_coords(square):\n",
    "    return square // 8, square % 8\n",
    "\n",
    "def coords_to_square(row, col):\n",
    "    return row * 8 + col\n",
    "\n",
    "\n",
    "def create_edges() -> torch.Tensor:\n",
    "\n",
    "    edge_index = []\n",
    "\n",
    "    def add_edges(moves, square, row, col):\n",
    "        for dr, dc in moves:\n",
    "            nr, nc = row + dr, col + dc\n",
    "            if 0 <= nr < 8 and 0 <= nc < 8:\n",
    "                target_square = coords_to_square(nr, nc)\n",
    "                edge_index.append([square, target_square])\n",
    "        \n",
    "    # Total nodes (squares)\n",
    "    num_nodes = 64\n",
    "\n",
    "    # Directions for knight moves\n",
    "    knight_moves = [\n",
    "        (-2, -1), (-2, 1), (-1, -2), (-1, 2),\n",
    "        (1, -2), (1, 2), (2, -1), (2, 1)\n",
    "    ]\n",
    "    \n",
    "    # Directions for queen moves (also covers other piece moves)\n",
    "    queen_moves = [\n",
    "        (dx, dy)\n",
    "        for dx in range(-7, 8) for dy in range(-7, 8)\n",
    "        if (dx == 0) != (dy == 0) or abs(dx) == abs(dy)\n",
    "    ]\n",
    "\n",
    "    # Add edges for each square\n",
    "    for square in range(num_nodes):\n",
    "        row, col = square_to_coords(square)\n",
    "        add_edges_for_square = partial(add_edges, square=square, col=col, row=row)\n",
    "\n",
    "        # loop through knight/queen moves\n",
    "        add_edges_for_square(knight_moves)\n",
    "        add_edges_for_square(queen_moves)\n",
    "\n",
    "    # Convert edge list to tensor\n",
    "    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "edge_index = create_edges()\n",
    "print(\"Edge Index:\", edge_index)\n",
    "print(\"Number of Edges:\", edge_index.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f6245417-1f16-4801-a03a-722cabfcdaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: -20\n",
      "Normalizd: tensor([-3.0445])\n",
      "Normalizd: tensor([-20.0000])\n",
      "Evaluation: -2000\n",
      "Normalizd: tensor([-7.6014])\n",
      "Normalizd: tensor([-1999.9999])\n",
      "Evaluation: -95040\n",
      "Normalizd: tensor([-11.4621])\n",
      "Normalizd: tensor([-95040.0156])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26244/3457003888.py:34: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  return np.sign(normalized_data) * (np.expm1(np.abs(normalized_data)))\n"
     ]
    }
   ],
   "source": [
    "def calculate_inverse_output(input_value: str) -> int:\n",
    "    \"\"\"\n",
    "    Define the range of the output values checkmate in 100 moves is equivalent \n",
    "    to +10 evaluation and mate in 1 move is equivalent to +75 evaluation\n",
    "    \"\"\"\n",
    "    max_output = 99_000\n",
    "    min_output = 1_000\n",
    "    \n",
    "    # Calculate the output using an inverse proportional formula\n",
    "    output = max_output - (input_value * (max_output - min_output) / 99)\n",
    "    return int(max(min_output, min(max_output, output)))  # Clamp output within range\n",
    "\n",
    "\n",
    "# there are evaluations in the range 1000 - 9999 which are unnecessary big -> might be good idea to normalize them\n",
    "def normalize_evaluation(evaluation: int) -> float:\n",
    "    \"\"\"\n",
    "    For centipawn evaluations, signed log transformation is often a good choice because:\n",
    "\n",
    "    It compresses large values.\n",
    "    It keeps smaller values distinguishable.\n",
    "    It ensures a smooth distribution of data for regression.\n",
    "\n",
    "    Why Normalize?\n",
    "\n",
    "    1. Gradient Stability: Large differences in target values can lead to large gradients during backpropagation, which may destabilize training or cause slower convergence.\n",
    "    2. Model Sensitivity: If the majority of your data lies in the 0–100 range but you also have outliers in the 1000–9999 range, the model might overfit or focus disproportionately on these outliers.\n",
    "    3. Consistency: Normalizing the data ensures all values are on a similar scale, making the model's predictions more interpretable.\n",
    "    \"\"\"\n",
    "    data = torch.tensor([evaluation], dtype=torch.float)\n",
    "    return torch.sign(data) * torch.log(torch.abs(data) + 1)\n",
    "\n",
    "\n",
    "def denormalize_evaluation(normalized_data: float) -> float:\n",
    "    return np.sign(normalized_data) * (np.expm1(np.abs(normalized_data)))\n",
    "\n",
    "\n",
    "def parse_evaluation(evaluation: str) -> int:\n",
    "    # in centipawns  \n",
    "    evaluation = evaluation.strip()\n",
    "    \n",
    "    # evaluate checkmates (e.g. #+63 / #-0)\n",
    "    if evaluation.startswith('#'):\n",
    "        evaluation = evaluation.lstrip('#+')\n",
    "\n",
    "        if evaluation.startswith('-'):\n",
    "            black_to_move = True\n",
    "        else:\n",
    "            black_to_move = False\n",
    "\n",
    "        evaluation = abs(int(evaluation))\n",
    "\n",
    "        if evaluation == 0:\n",
    "            evaluation = 10_000\n",
    "        else:\n",
    "            evaluation = calculate_inverse_output(evaluation)\n",
    "\n",
    "        return -1*evaluation if black_to_move else evaluation\n",
    "\n",
    "    return int(evaluation)\n",
    "\n",
    "\n",
    "def parse_chess_data(chess_data: str) -> list:\n",
    "    \"\"\"\n",
    "    Parse fen and evaluation from the raw data\n",
    "    \n",
    "    Parameters: \n",
    "        chess_data:\n",
    "    r2qkb1r/pp1b1ppp/1n2p3/n2pP3/3P1P2/1BN2N2/PP2Q1PP/R1B1K2R w KQkq - 4 12,+115  \n",
    "    \"\"\"\n",
    "    fen, evaluation = chess_data.split(',')\n",
    "    return FenParser(fen), parse_evaluation(evaluation) \n",
    "\n",
    "\n",
    "evaluation = -20\n",
    "print(f\"Evaluation: {evaluation}\")\n",
    "normalizd = normalize_evaluation(evaluation)\n",
    "print(f\"Normalizd: {normalizd}\")\n",
    "print(f\"Normalizd: {denormalize_evaluation(normalizd)}\")\n",
    "\n",
    "evaluation = -2000\n",
    "print(f\"Evaluation: {evaluation}\")\n",
    "normalizd = normalize_evaluation(evaluation)\n",
    "print(f\"Normalizd: {normalizd}\")\n",
    "print(f\"Normalizd: {denormalize_evaluation(normalizd)}\")\n",
    "\n",
    "evaluation = \"#-4\"\n",
    "evaluation = parse_evaluation(evaluation)\n",
    "print(f\"Evaluation: {evaluation}\")\n",
    "normalizd = normalize_evaluation(evaluation)\n",
    "print(f\"Normalizd: {normalizd}\")\n",
    "print(f\"Normalizd: {denormalize_evaluation(normalizd)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "33423742-64be-4d6d-8a02-4edabb720bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.FenParser at 0x7fae3279d580>, -84151)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_chess_data(\"r2qkb1r/pp1b1ppp/1n2p3/n2pP3/3P1P2/1BN2N2/PP2Q1PP/R1B1K2R w KQkq - 4 12,+115\")\n",
    "parse_chess_data(\"r2qkb1r/pp1b1ppp/1n2p3/n2pP3/3P1P2/1BN2N2/PP2Q1PP/R1B1K2R w KQkq - 4 12,#-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "708c515b-41bd-4633-b342-b4debe172915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node features: these represent the pieces on the squares (nodes)\n",
    "\n",
    "def piece_to_tensor(piece: str) -> torch.tensor:\n",
    "    piece = chess.Piece.from_symbol(piece)\n",
    "    node_feature = torch.zeros(13)\n",
    "    node_feature[piece.__hash__() + 1] = 1\n",
    "    return node_feature\n",
    "\n",
    "def fen_to_node_features(fen):    \n",
    "    num_nodes = 64\n",
    "    node_features = torch.zeros((num_nodes, 13))\n",
    "    \n",
    "    for i,row in enumerate(list(reversed(fen.parse()))):\n",
    "        for j, square in enumerate(row):\n",
    "            if len(square.strip()) > 0:\n",
    "                node_features[chess.square(j, i)] = piece_to_tensor(square)\n",
    "\n",
    "            # whose turn it's to move\n",
    "            # NOTE: this could be encoded also as a global feature for the whole graph\n",
    "            # but we'll do it like this for simplicity\n",
    "            if fen.white_to_move():\n",
    "                node_features[chess.square(j, i)][0] = 1\n",
    "\n",
    "    return node_features\n",
    "\n",
    "node_features = fen_to_node_features(FenParser(fen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "12c293a4-bb16-431b-b3b5-c3ee3507a63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edge features: these represent legal moves between squares (nodes)\n",
    "\n",
    "def alphabetical_distance(char1, char2):\n",
    "    return abs(ord(char1.lower()) - ord(char2.lower()))\n",
    "\n",
    "def fen_to_edge_features(fen_parser, edges):\n",
    "\n",
    "    # initialize edge feature tensor\n",
    "    edge_features = torch.zeros((edges.size(1), 17))\n",
    "\n",
    "    white_to_move = fen_parser.white_to_move()\n",
    "\n",
    "    # 1. map pieces to squares\n",
    "    pieces = {}\n",
    "    for i,row in enumerate(list(reversed(fen_parser.parse()))):\n",
    "        for j, square in enumerate(row):\n",
    "            if len(square.strip()) > 0:\n",
    "                pieces[chess.square(j, i)] = chess.Piece.from_symbol(square)\n",
    "\n",
    "    moves = 0\n",
    "    promoted = []\n",
    "    promote = False\n",
    "    \n",
    "    for move in fen_parser.legal_moves():\n",
    "\n",
    "        # move is in uci format (e.g. e2e4)\n",
    "        moves += 1\n",
    "        move = str(move)\n",
    "\n",
    "        # pawn promotions looks like this: a7b8Q (only moves where length > 4)\n",
    "        # there is alway 4 possible promotions but we are interested only from\n",
    "        # queen promotions for now, so we skip the rest of the possible promotions\n",
    "        if len(move) > 4:\n",
    "            move = move[:4]\n",
    "            if move in promoted:\n",
    "                continue\n",
    "            promoted.append(move)\n",
    "            promote = True\n",
    "\n",
    "        start, end = chess.parse_square(move[:2]), chess.parse_square(move[2:])\n",
    "\n",
    "        # calculate correct edge index from start/end\n",
    "        value_pair = torch.tensor([start, end])  \n",
    "        edge = torch.where((edges[0] == value_pair[0]) & (edges[1] == value_pair[1]))[0].item()\n",
    "\n",
    "        # legal move\n",
    "        edge_features[edge][0] = 1\n",
    "\n",
    "        # edge length\n",
    "        edge_features[edge][1] = abs(int(move[1]) - int(move[3]))\n",
    "        edge_features[edge][2] = alphabetical_distance(move[0], move[2])\n",
    "\n",
    "        # what piece is moved\n",
    "        if pieces[start].piece_type == 1:\n",
    "            # pawn\n",
    "            if white_to_move:\n",
    "                edge_features[edge][3] = 1\n",
    "            else:\n",
    "                edge_features[edge][4] = 1\n",
    "        elif pieces[start].piece_type == 6:\n",
    "            # king\n",
    "            if white_to_move:\n",
    "                edge_features[edge][5] = 1\n",
    "            else:\n",
    "                edge_features[edge][6] = 1\n",
    "        elif pieces[start].piece_type == 5:\n",
    "            # queen\n",
    "            edge_features[edge][7] = 1\n",
    "        elif pieces[start].piece_type == 2:\n",
    "            # knight\n",
    "            edge_features[edge][8] = 1\n",
    "        elif pieces[start].piece_type == 3:\n",
    "            # bishop\n",
    "            edge_features[edge][9] = 1\n",
    "        elif pieces[start].piece_type == 4:\n",
    "            # rook\n",
    "            edge_features[edge][10] = 1\n",
    "        else:\n",
    "            assert False, f\"Did not recognize piece symbol: {pieces[start].piece_type}\"\n",
    "        \n",
    "        # castling\n",
    "        if move == \"e1g1\" and fen_parser.can_castle('w', 'k'):\n",
    "            edge_features[edge][12] = 1\n",
    "        if move == \"e1c1\" and fen_parser.can_castle('w', 'q'):\n",
    "            edge_features[edge][13] = 1\n",
    "        if move == \"e8g8\" and fen_parser.can_castle('b', 'k'):\n",
    "            edge_features[edge][14] = 1\n",
    "        if move == \"e8c8\" and fen_parser.can_castle('b', 'q'):\n",
    "            edge_features[edge][15] = 1\n",
    "        \n",
    "        # pawn promotion \n",
    "        if promote:\n",
    "            edge_features[edge][16] = 1\n",
    "            promote = False\n",
    "     \n",
    "        #print(edge_features[edge])\n",
    "\n",
    "    return edge_features\n",
    "\n",
    "\n",
    "edge_features = fen_to_edge_features(FenParser(fen), create_edges())\n",
    "edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "927a4b40-144d-4624-b378-4f3ac49bc9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[64, 13], edge_index=[2, 1856], edge_attr=[1856, 17])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create the pytorch_geometric data object\n",
    "graph = Data(\n",
    "    x=fen_to_node_features(FenParser(fen)), \n",
    "    edge_index=edge_index,\n",
    "    edge_attr=fen_to_edge_features(FenParser(fen), create_edges())\n",
    ")\n",
    "\n",
    "print(graph)\n",
    "torch.save(graph, \"test_graph.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8c6769b2-009a-4333-bc92-a8cb54f13639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one graph is ~150kb, so creating 13M of these takes about 2TB disk space\n",
    "# -> create graphs on the fly for now\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "def create_graph_from_fen(fen_parser: str, edges) -> Data:\n",
    "    return Data(\n",
    "        x=fen_to_node_features(fen_parser), \n",
    "        edge_index=edges,\n",
    "        edge_attr=fen_to_edge_features(fen_parser, edges)\n",
    "    )\n",
    "\n",
    "\n",
    "class FENDataset(Dataset):\n",
    "    def __init__(self, fen_file, transform=None, pre_transform=None):\n",
    "        super().__init__(None, transform, pre_transform)\n",
    "        with open(fen_file, 'r') as f:\n",
    "            self.fen_list = [line.strip() for line in f]\n",
    "\n",
    "        self.edges = create_edges()\n",
    "        self.cache = {}\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.fen_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "            \n",
    "        fen_str = self.fen_list[idx]\n",
    "        fen_parser, evaluation = parse_chess_data(fen_str)\n",
    "        graph = create_graph_from_fen(fen_parser, self.edges)  \n",
    "\n",
    "        # store the evaluation\n",
    "        graph.y = normalize_evaluation(evaluation)\n",
    "\n",
    "        self.cache[idx] = graph\n",
    "        \n",
    "        return graph\n",
    "\n",
    "#dataset = FENDataset('chessData.csv')\n",
    "dataset = FENDataset('first_10k_evaluations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c16dd784-a1ee-4dc7-ba51-c6c8726f9c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "00fffd06-7b28-4f4b-9ca4-358d82df3b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features=13, hidden_size=64, target_size=1):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.target_size = target_size\n",
    "        self.convs = [GATConv(self.num_features, self.hidden_size, edge_dim=17),\n",
    "                      GATConv(self.hidden_size, self.hidden_size, edge_dim=17)]\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x=x, edge_index=edge_index, edge_attr=edge_attr) # adding edge features here!\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.convs[-1](x=x, edge_index=edge_index, edge_attr=edge_attr) # edge features here as well\n",
    "\n",
    "        # Global mean pooling: aggregate node features to get a graph-level embedding\n",
    "        x = global_mean_pool(x, batch)  # 'batch' tensor tells which nodes belong to which graph\n",
    "\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "80e9eb89-bed2-4909-87df-e4db0291116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph-level output: tensor([0.0987], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "\n",
    "# Custom Message Passing Layer\n",
    "class CustomMessagePassing(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, aggr='add'):\n",
    "        super(CustomMessagePassing, self).__init__(aggr=aggr)\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "        self.edge_encoder = nn.Linear(17, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        if edge_attr is not None:\n",
    "            edge_embedding = self.edge_encoder(edge_attr)\n",
    "            return self.linear(x_j) + edge_embedding\n",
    "        return self.linear(x_j)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "# Attention-Based Global Pooling\n",
    "class AttentionGlobalPooling(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(AttentionGlobalPooling, self).__init__()\n",
    "        self.attention_mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute attention scores\n",
    "        attention_scores = self.attention_mlp(x)  # [num_nodes, 1]\n",
    "        attention_scores = F.softmax(attention_scores, dim=0)  # Normalize across all nodes\n",
    "\n",
    "        # Apply attention scores to node features\n",
    "        x_weighted = x * attention_scores  # Element-wise multiplication\n",
    "\n",
    "        # Aggregate node features (sum)\n",
    "        graph_embedding = x_weighted.sum(dim=0)  # [in_channels]\n",
    "        return graph_embedding\n",
    "\n",
    "\n",
    "# EPD GNN Architecture for a Single Graph\n",
    "class AttentionEPDGNN(nn.Module):\n",
    "    def __init__(self, in_channels=13, hidden_channels=64, out_channels=1, num_processors=2):\n",
    "        super(AttentionEPDGNN, self).__init__()\n",
    "\n",
    "        # Encoder: Node feature transformation\n",
    "        # Raw node features (like piece type, color, position) are usually sparse or simple.\n",
    "        # The encoder learns a richer, task-specific representation in the hidden embedding space.\n",
    "        self.encoder = nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # Processor: Stack of message-passing layers\n",
    "        self.processors = nn.ModuleList([\n",
    "            CustomMessagePassing(hidden_channels, hidden_channels)\n",
    "            for _ in range(num_processors)\n",
    "        ])\n",
    "\n",
    "        # Attention-based global pooling\n",
    "        self.attention_pooling = AttentionGlobalPooling(hidden_channels, hidden_channels // 2)\n",
    "\n",
    "        # Decoder: Fully connected layers for graph-level output\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels // 2, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        # Encoder: Transform node features\n",
    "        x = self.encoder(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Processor: Message passing layers\n",
    "        for processor in self.processors:\n",
    "            x = processor(x, edge_index, edge_attr=edge_attr)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # Attention-based pooling for aggregating graph-wide information\n",
    "        graph_embedding = self.attention_pooling(x)\n",
    "\n",
    "        # Decoder: Predict graph-level output\n",
    "        out = self.decoder(graph_embedding)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example Usage with a Single Graph\n",
    "\n",
    "# 10 nodes with 8-dimensional features\n",
    "x = torch.rand((10, 8))  \n",
    "\n",
    "# Random edge indexes\n",
    "edge_index = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                           [1, 0, 3, 2, 5, 4, 7, 6, 9, 8]], dtype=torch.long)\n",
    "# Random edge features\n",
    "edge_attr = torch.rand((10, 17))\n",
    "\n",
    "# Create model\n",
    "model = AttentionEPDGNN(in_channels=8, hidden_channels=64, out_channels=1)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x, edge_index, edge_attr=edge_attr)\n",
    "print(\"Graph-level output:\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ee2231ad-115d-4425-af5b-2875ddb4c8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[2048, 13], edge_index=[2, 59392], edge_attr=[59392, 17], y=[32], batch=[2048], ptr=[33])\n"
     ]
    }
   ],
   "source": [
    "# Separate data to training, validation and testing sets\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define lengths for train, validation, and test\n",
    "train_len = int(0.7 * len(dataset))  # 70% for training\n",
    "val_len = int(0.15 * len(dataset))   # 15% for validation\n",
    "test_len = len(dataset) - train_len - val_len  # 15% for testing\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    batch = batch.to('cpu')\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1f6507ce-9308-4792-995d-e308c79fde8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 22.2782, Accuracy: 0.00%\n",
      "Validation Loss: 21.3796, Accuracy: 0.00%\n",
      "Epoch 2/2, Loss: 21.0477, Accuracy: 0.00%\n",
      "Validation Loss: 21.4662, Accuracy: 0.00%\n",
      "Test Loss: 20.3981, Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "in_channels = 13\n",
    "hidden_channels = 64\n",
    "num_iterations = 3\n",
    "\n",
    "# Model\n",
    "model = AttentionEPDGNN(\n",
    "    in_channels=in_channels, \n",
    "    hidden_channels=hidden_channels, \n",
    "    out_channels=1\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Mean Squared Error for regression (centipawn evaluation)\n",
    "criterion = torch.nn.MSELoss() \n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)  # Move batch to device (GPU or CPU)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Get model predictions\n",
    "        output = model(batch.x, batch.edge_index, edge_attr=batch.edge_attr)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, batch.y) \n",
    "        \n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Optimizer step\n",
    "        \n",
    "        # Track loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        predicted = output.squeeze()\n",
    "        correct += (predicted == batch.y).sum().item()\n",
    "        total += batch.num_graphs  # Total number of graphs in the batch\n",
    "    \n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy*100:.2f}%')\n",
    "    \n",
    "    # Validation step (no gradients needed)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed during validation\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            output = model(batch.x, batch.edge_index, edge_attr=batch.edge_attr)\n",
    "            loss = criterion(output, batch.y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Accuracy calculation\n",
    "            predicted = output.squeeze()\n",
    "            total += batch.num_graphs\n",
    "            correct += (predicted == batch.y).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%')\n",
    "\n",
    "# Test after training is done\n",
    "model.eval()  # Set model to evaluation mode for testing\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        output = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        loss = criterion(output, batch.y)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Accuracy calculation\n",
    "        predicted = output.squeeze()\n",
    "        total += batch.num_graphs\n",
    "        correct += (predicted == batch.y).sum().item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = correct / total\n",
    "\n",
    "print(f'Test Loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0b176158-6ba7-4ab3-b55f-4db6ea242117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/laineo/Work/notebooks\n",
      "Checkpoint saved to: model_checkpoint_001.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "checkpoint = 'model_checkpoint_001.pt'\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch + 1,                  # Save the current epoch number\n",
    "    'model_state_dict': model.state_dict(),  # Save the model's weights\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # Save the optimizer's state\n",
    "    'loss': loss.item(),                 # Save the current loss value\n",
    "}, checkpoint)  # Specify the filename\n",
    "\n",
    "print(f\"Checkpoint saved to: {checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3af6c-92c2-4cf0-9f28-1acf699c2b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
